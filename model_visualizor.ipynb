{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def make_dot(var, params=None):\n",
    "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n",
    "    Blue nodes are the Variables that require grad, orange are Tensors\n",
    "    saved for backward in torch.autograd.Function\n",
    "    Args:\n",
    "        var: output Variable\n",
    "        params: dict of (name, Variable) to add names to node that\n",
    "            require grad (TODO: make optional)\n",
    "    \"\"\"\n",
    "    if params is not None:\n",
    "        assert isinstance(params.values()[0], Variable)\n",
    "        param_map = {id(v): k for k, v in params.items()}\n",
    "\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "\n",
    "    def size_to_str(size):\n",
    "        return '('+(', ').join(['%d' % v for v in size])+')'\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if torch.is_tensor(var):\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
    "            elif hasattr(var, 'variable'):\n",
    "                u = var.variable\n",
    "                name = param_map[id(u)] if params is not None else ''\n",
    "                node_name = '%s\\n %s' % (name, size_to_str(u.size()))\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'next_functions'):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, 'saved_tensors'):\n",
    "                for t in var.saved_tensors:\n",
    "                    dot.edge(str(id(t)), str(id(var)))\n",
    "                    add_nodes(t)\n",
    "    add_nodes(var.grad_fn)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CombinedTransform' object has no attribute 'grad_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-39af442873a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mweights_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dry/saved/EncdedWideResnetFIXED/state_dict_highscore'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-678f234a4341>\u001b[0m in \u001b[0;36mmake_dot\u001b[0;34m(var, params)\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0madd_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0madd_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/pytorch/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 262\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CombinedTransform' object has no attribute 'grad_fn'"
     ]
    }
   ],
   "source": [
    "import colorplot as cp\n",
    "net, train_producer = cp._setup(\n",
    "    label_path   = 'data/raw/training2017/REFERENCE.csv',\n",
    "    data_path    = 'data/raw/training2017/',\n",
    "    weights_path = 'dry/saved/EncdedWideResnetFIXED/state_dict_highscore'\n",
    ")\n",
    "g = make_dot(net)\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedTransform (\n",
       "  (models): ModuleList (\n",
       "    (0): SkipFCN (\n",
       "      (pool): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (activation): SELU (\n",
       "      )\n",
       "      (conv1): Conv1d(16, 16, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv1d(16, 16, kernel_size=(9,), stride=(1,), padding=(8,), dilation=(2,), bias=False)\n",
       "      (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv1d(16, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "      (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv4): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(8,), dilation=(2,), bias=False)\n",
       "      (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv5): Conv1d(48, 64, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "      (bn5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv6): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(8,), dilation=(2,), bias=False)\n",
       "      (bn6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv7): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(16,), dilation=(4,), bias=False)\n",
       "      (bn7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv8): Conv1d(80, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "      (bn8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv9): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(8,), dilation=(2,), bias=False)\n",
       "      (bn9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv10): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(16,), dilation=(4,), bias=False)\n",
       "      (bn10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv11): Conv1d(144, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "      (bn11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv12): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(8,), dilation=(2,), bias=False)\n",
       "      (bn12): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv13): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(8,), dilation=(2,), bias=False)\n",
       "      (bn13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (drop1): Dropout (p = 0.5, inplace)\n",
       "      (logit): Conv1d(128, 3, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): EncodeWideResNetFIXED (\n",
       "      (nonlin): SELU (\n",
       "      )\n",
       "      (encoder): Sequential (\n",
       "        (0): Conv1d(1, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (2): SELU (\n",
       "        )\n",
       "        (3): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): Conv1d(16, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (6): SELU (\n",
       "        )\n",
       "        (7): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (8): Conv1d(32, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (10): SELU (\n",
       "        )\n",
       "        (11): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (12): Conv1d(64, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "        (14): SELU (\n",
       "        )\n",
       "        (15): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (resnet): Sequential (\n",
       "        (0): ConvModule (\n",
       "          (residuals): Sequential (\n",
       "            (0): DilatedBlock (\n",
       "              (nonlin): SELU (\n",
       "              )\n",
       "              (block): Sequential (\n",
       "                (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "                (1): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "                (2): SELU (\n",
       "                )\n",
       "                (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "                (4): Dropout (p = 0.5, inplace)\n",
       "                (5): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(8,), dilation=(2,), bias=False)\n",
       "                (6): SELU (\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ConvModule (\n",
       "          (residuals): Sequential (\n",
       "            (0): DilatedBlock (\n",
       "              (nonlin): SELU (\n",
       "              )\n",
       "              (block): Sequential (\n",
       "                (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "                (1): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "                (2): SELU (\n",
       "                )\n",
       "                (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "                (4): Dropout (p = 0.5, inplace)\n",
       "                (5): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(8,), dilation=(2,), bias=False)\n",
       "                (6): SELU (\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ConvModule (\n",
       "          (residuals): Sequential (\n",
       "            (0): DilatedBlock (\n",
       "              (nonlin): SELU (\n",
       "              )\n",
       "              (block): Sequential (\n",
       "                (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "                (1): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
       "                (2): SELU (\n",
       "                )\n",
       "                (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "                (4): Dropout (p = 0.5, inplace)\n",
       "                (5): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(8,), dilation=(2,), bias=False)\n",
       "                (6): SELU (\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (logit): Conv1d(128, 3, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential (\n",
       "    (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (1): SELU (\n",
       "    )\n",
       "    (2): Conv1d(256, 3, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.0.bias torch.Size([256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "from_numpy expects an np.ndarray but got torch.cuda.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-009d18c3fe89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: from_numpy expects an np.ndarray but got torch.cuda.FloatTensor"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "params = th.load('dry/saved/EncdedWideResnetFIXED/state_dict_highscore')\n",
    "\n",
    "# convert numpy arrays to torch Variables\n",
    "for k in sorted(params.keys()):\n",
    "    v = params[k]\n",
    "    print(k, v.shape)\n",
    "    params[k] = Variable(torch.from_numpy(v), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-57226824bb30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "def define_model(params):\n",
    "    def conv2d(input, params, base, stride=1, pad=0):\n",
    "        return F.conv2d(input, params[base + '.weight'],\n",
    "                        params[base + '.bias'], stride, pad)\n",
    "\n",
    "    def group(input, params, base, stride, n):\n",
    "        o = input\n",
    "        for i in range(0,n):\n",
    "            b_base = ('%s.block%d.conv') % (base, i)\n",
    "            x = o\n",
    "            o = conv2d(x, params, b_base + '0', pad=1, stride=i==0 and stride or 1)\n",
    "            o = F.relu(o)\n",
    "            o = conv2d(o, params, b_base + '1', pad=1)\n",
    "            if i == 0 and stride != 1:\n",
    "                o += F.conv2d(x, params[b_base + '_dim.weight'], stride=stride)\n",
    "            else:\n",
    "                o += x\n",
    "            o = F.relu(o)\n",
    "        return o\n",
    "    \n",
    "    # determine network size by parameters\n",
    "    blocks = [sum([re.match('group%d.block\\d+.conv0.weight'%j, k) is not None\n",
    "                   for k in params.keys()]) for j in range(4)]\n",
    "\n",
    "    def f(input, params):\n",
    "        o = F.conv2d(input, params['conv0.weight'], params['conv0.bias'], 2, 3)\n",
    "        o = F.relu(o)\n",
    "        o = F.max_pool2d(o, 3, 2, 1)\n",
    "        o_g0 = group(o, params, 'group0', 1, blocks[0])\n",
    "        o_g1 = group(o_g0, params, 'group1', 2, blocks[1])\n",
    "        o_g2 = group(o_g1, params, 'group2', 2, blocks[2])\n",
    "        o_g3 = group(o_g2, params, 'group3', 2, blocks[3])\n",
    "        o = F.avg_pool2d(o_g3, 7, 1, 0)\n",
    "        o = o.view(o.size(0), -1)\n",
    "        o = F.linear(o, params['fc.weight'], params['fc.bias'])\n",
    "        return o\n",
    "    return f\n",
    "\n",
    "f = define_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
