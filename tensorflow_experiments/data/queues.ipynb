{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files: 08527   \n",
      "Reading successful!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env ipython\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io as io\n",
    "\n",
    "all_label_dict = {\n",
    "    'N':0,\n",
    "    'A':1,\n",
    "    'O':2,\n",
    "    '~':3\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "all_label = []\n",
    "all_lens = []\n",
    "annotations = open('./raw/REFERENCE.csv', 'r').read().splitlines()\n",
    "for i, line in enumerate(annotations):\n",
    "    fname, all_label_str = line.split(',')\n",
    "    \n",
    "    x = io.loadmat('./raw/'+fname+'.mat')['val']\\\n",
    "        .astype(np.float32).squeeze()\n",
    "    \n",
    "    # [0, 1]\n",
    "    # x -= x.min()\n",
    "    # x /= x.max()\n",
    "    \n",
    "    # [-1, 1]\n",
    "    # x -= x.min()\n",
    "    # x /= x.max()\n",
    "    # x *= 2\n",
    "    # x -= 0\n",
    "    \n",
    "    # Normal\n",
    "    x -= x.mean()\n",
    "    x /= x.std()\n",
    "    \n",
    "    all_data.append(x)\n",
    "    \n",
    "    y = all_label_dict[all_label_str]\n",
    "    all_label.append(y)\n",
    "    \n",
    "    all_lens.append(len(x))\n",
    "    if i%50==0: \n",
    "        print('\\rReading files: %05d   ' % i, end='', flush=True)\n",
    "\n",
    "print('\\rReading files: %05d   ' % i, end='', flush=True)\n",
    "    \n",
    "assert(len(all_label) == len(all_data) == len(all_lens))\n",
    "print('\\nReading successful!')\n",
    "all_data_size = len(all_data)\n",
    "\n",
    "# No problem with different lengths\n",
    "# Using np.array because slice indexing does not copy the all_data\n",
    "# While native python slicing does\n",
    "all_data = np.array(all_data)\n",
    "all_label = np.array(all_label)\n",
    "all_lens = np.array(all_lens)\n",
    "class_weights = np.histogram(all_label, bins=len(all_label_dict))[0]/len(all_label)\n",
    "all_weight = class_weights[all_label]\n",
    "\n",
    "def shuffle():\n",
    "    global all_data\n",
    "    global all_label\n",
    "    global all_lens\n",
    "    p = np.random.permutation(all_data_size)\n",
    "    # Using fancy indexing for Unison Shuffle\n",
    "    all_data = all_data[p]\n",
    "    all_label = all_label[p]\n",
    "    all_lens = all_lens[p]\n",
    "    \n",
    "def join_samples(sample_list, sample_all_lens):\n",
    "    res = np.zeros((len(sample_list), sample_all_lens.max(), 1))\n",
    "    for idx, (sample, l) in enumerate(zip(sample_list, sample_all_lens)):\n",
    "        res[idx, :l] = sample[None, :, None]\n",
    "    return res \n",
    "\n",
    "def random_batch(batch_size=8):\n",
    "    shuffle()\n",
    "    n = batch_size\n",
    "    for i in range(0, all_data_size, batch_size):\n",
    "        yield all_data[i:i+n], all_label[i:i+n], all_lens[i:i+n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6043621 ,  0.6043621 ,  0.6043621 ,  0.09040807,  0.29983583])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weight[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6043621   0.09040807  0.29983583  0.005394  ] [0 0 0 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.6043621 ,  0.6043621 ,  0.6043621 ,  0.09040807,  0.29983583])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = np.histogram(all_label, bins=len(all_label_dict))[0]/len(all_label)\n",
    "print(class_weights, all_label[:5])\n",
    "class_weights[all_label[:5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_input_data = tf.placeholder(tf.float32, [None], name='in_node')\n",
    "q = tf.PaddingFIFOQueue(capacity=32, dtypes=[tf.float32], shapes=[x_input_data.shape])\n",
    "enqueue_op = q.enqueue(x_input_data)\n",
    "padded_batch = q.dequeue_many(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session started\n",
      "Enqueued: 9\n",
      "done\n",
      "(2, 9000)\n",
      "(2, 18000)\n",
      "(2, 18000)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('session started')\n",
    "    for i, sample in enumerate(all_data[:10]):\n",
    "        enqueue_op.run({x_input_data:sample})\n",
    "        print('\\rEnqueued: %d'%i, end='', flush=True)\n",
    "    print('\\ndone')\n",
    "    for i in range(3): \n",
    "        print(padded_batch.eval().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_1:0' shape=(9000,) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor(all_data[:2].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# tf.train.batch usage\n",
    "\n",
    "* Must use new queue\n",
    "* The `train.batch`  uses threading, and has its own queue\n",
    "* Feeding only with a **new queue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_input_data = tf.placeholder(tf.float32, [None], name='in_node')\n",
    "q = tf.FIFOQueue(capacity=all_data_size, dtypes=tf.float32)\n",
    "#x_input_data = tf.Print(x_input_data, data=[x_input_data], message=\"Raw inputs data generated:\", summarize=6)\n",
    "enqueue_op = q.enqueue(x_input_data)\n",
    "input = q.dequeue()\n",
    "input.set_shape([None])\n",
    "batched_data = tf.train.batch(\n",
    "    tensors=[input], \n",
    "    batch_size=3, \n",
    "    dynamic_pad=True, \n",
    "    enqueue_many=False\n",
    ")\n",
    "qsize = q.size()\n",
    "numberOfThreads = 1 \n",
    "qr = tf.train.QueueRunner(q, [enqueue_op] * numberOfThreads)\n",
    "tf.train.add_queue_runner(qr)\n",
    "#input = tf.Print(input, data=[q.size()], message=\"Nb elements left:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session started\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Run call was cancelled\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-148b52ea1225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#print('Current size of queue:', q.size().eval())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     '''\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx_input_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m           \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('Session started')\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    '''for i, sample in enumerate(all_data):\n",
    "        sess.run(enqueue_op, {x_input_data:sample})\n",
    "        print('\\rEnqueued: %d'%i, end='', flush=True)\n",
    "        #print('Current size of queue:', q.size().eval())\n",
    "    '''\n",
    "    sess.run(batched_data, {x_input_data:all_data}).shape\n",
    "    print()\n",
    "    print('done')\n",
    "    #while not coord.should_stop():\n",
    "    '''res = sess.run(batched_data)\n",
    "    print(qsize.eval())\n",
    "    res = sess.run(batched_data)\n",
    "    print(qsize.eval())\n",
    "    res = sess.run(batched_data)  \n",
    "    print(qsize.eval())\n",
    "    '''\n",
    "    #print(res.shape)             \n",
    "    #print('Current size of queue:', q.size().eval())\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "    \n",
    "#res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# We simulate some raw input data\n",
    "# let's start with only 3 samples of 1 data point\n",
    "x_input_data = tf.random_normal([3], mean=-1, stddev=4)\n",
    "\n",
    "# We build a FIFOQueue inside the graph \n",
    "# You can see it as a waiting line that holds waiting data\n",
    "# In this case, a line with only 3 positions\n",
    "q = tf.FIFOQueue(capacity=3, dtypes=tf.float32)\n",
    "\n",
    "# We need an operation that will actually fill the queue with our data\n",
    "# \"enqueue_many\" slices \"x_input_data\" along the 0th dimension to make multiple queue elements\n",
    "enqueue_op = q.enqueue_many(x_input_data) # <- x1 - x2 -x3 |\n",
    "\n",
    "# We need a dequeue op to get the next elements in the queue following the FIFO policy.\n",
    "input = q.dequeue() \n",
    "# The input tensor is the equivalent of a placeholder now \n",
    "# but directly connected to the data sources in the graph\n",
    "\n",
    "# Each time we use the input tensor, we print the number of elements left\n",
    "# in the queue\n",
    "input = tf.Print(input, data=[q.size()], message=\"Nb elements left:\")\n",
    "\n",
    "# fake graph: START\n",
    "y = input + 1\n",
    "# fake graph: END \n",
    "\n",
    "# We start the session as usual\n",
    "with tf.Session() as sess:\n",
    "    print('start')\n",
    "    # We first run the enqueue_op to load our data into the queue\n",
    "    sess.run(enqueue_op)\n",
    "    # Now, our queue holds 3 elements, it's full. \n",
    "    # We can start to consume our data\n",
    "    sess.run(y)\n",
    "    sess.run(y) \n",
    "    sess.run(y) \n",
    "    print('end')\n",
    "    # Now our queue is empty, if we call it again, our program will hang right here\n",
    "    # waiting for the queue to be filled by at least one more datum\n",
    "    #sess.run(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "enqueue_op.run({x_input_data:[0, 1, 2, 3, 4, 5, 6]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 2, 0, 0],\n",
       "       [1, 2, 3, 0],\n",
       "       [1, 2, 3, 4]], dtype=int32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.range(1, 10, name=\"x\")\n",
    "\n",
    "# A queue that outputs 0,1,2,3,...\n",
    "range_q = tf.train.range_input_producer(limit=5, shuffle=False)\n",
    "slice_end = range_q.dequeue()\n",
    "\n",
    "# Slice x to variable length, i.e. [0], [0, 1], [0, 1, 2], ....\n",
    "y = tf.slice(x, [0], [slice_end], name=\"y\")\n",
    "\n",
    "batched_data = tf.train.batch(\n",
    "    tensors=[y],\n",
    "    batch_size=5,\n",
    "    dynamic_pad=True,\n",
    "    name=\"y_batch\",\n",
    "    enqueue_many=False\n",
    ")\n",
    "\n",
    "#res = tf.contrib.learn.run_n({\"y\": batched_data}, n=1, feed_dict=None)\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    res = sess.run(batched_data)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'y_batch_1:0' shape=(5, ?) dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "initial_value must have a shape specified: Tensor(\"Placeholder_1:0\", shape=(?,), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e734060e6d33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtensor_list_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtensor_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_list_initializer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-e734060e6d33>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtensor_list_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtensor_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_list_initializer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope)\u001b[0m\n\u001b[1;32m    224\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m           expected_shape=expected_shape)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape)\u001b[0m\n\u001b[1;32m    327\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minitial_value_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             raise ValueError(\"initial_value must have a shape specified: %s\" %\n\u001b[0;32m--> 329\u001b[0;31m                              self._initial_value)\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# Assigns initial value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: initial_value must have a shape specified: Tensor(\"Placeholder_1:0\", shape=(?,), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tensor_list_initializer = [tf.placeholder(tf.float32, [None])]*20\n",
    "tensor_list = [tf.Variable(init) for init in tensor_list_initializer]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecords\n",
    "\n",
    "Write **`tf.train.SequenceExample`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_example(sequence, label, weight):\n",
    "    # The object we return\n",
    "    ex = tf.train.SequenceExample()\n",
    "    # A non-sequential feature of our example\n",
    "    sequence_length = len(sequence)\n",
    "    ex.context.feature['length'].int64_list.value.append(sequence_length)\n",
    "    ex.context.feature['label'].int64_list.value.append(label)\n",
    "    ex.context.feature['weight'].float_list.value.append(weight)\n",
    "    \n",
    "    fl_val = ex.feature_lists.feature_list['data']\n",
    "    for token in sequence:\n",
    "        fl_val.feature.add().float_list.value.append(token)\n",
    "\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_TFRecord(data, label, weight, fname='train'):\n",
    "    with open(fname + '.TFRecord', 'w') as fp:\n",
    "        writer = tf.python_io.TFRecordWriter(fp.name)\n",
    "        print('Sampling...')\n",
    "        for i, (x, y, w) in enumerate(zip(data, label, weight)):\n",
    "            ex = make_example(x, y, w)\n",
    "            \n",
    "            writer.write(ex.SerializeToString())\n",
    "            print('\\r%05d'%i, end=' ', flush=True)\n",
    "        writer.close()\n",
    "        print(\"\\nWrote to {}\".format(fp.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling...\n",
      "00019  \n",
      "Wrote to train.TFRecord\n"
     ]
    }
   ],
   "source": [
    "write_TFRecord(all_data[:20], all_label[:20], all_weight[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def parse_TFRecords_example(filename_queue):\n",
    "    # Define how to parse the example\n",
    "    \n",
    "    reader = tf.TFRecordReader()\n",
    "    _, example = reader.read(filename_queue)\n",
    "    \n",
    "    context_features = {\n",
    "        'length': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "        'label': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "        'weight': tf.FixedLenFeature([], dtype=tf.float32),\n",
    "    }\n",
    "    sequence_features = {\n",
    "        'data': tf.FixedLenSequenceFeature([], dtype=tf.float32)\n",
    "    }\n",
    "    con_parsed, seq_parsed = tf.parse_single_sequence_example(\n",
    "        serialized=example,\n",
    "        context_features=context_features,\n",
    "        sequence_features=sequence_features\n",
    "    )\n",
    "    res = (seq_parsed['data'], \n",
    "        con_parsed['length'], \n",
    "        con_parsed['label'], \n",
    "        con_parsed['weight'])\n",
    "    return res\n",
    "parse_example = parse_TFRecords_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sess started\n",
      "fetched batch: 0\n",
      "fetched batch: 1\n",
      "fetched batch: 2\n",
      "fetched batch: 3\n",
      "fetched batch: 4\n"
     ]
    }
   ],
   "source": [
    "filename_queue = tf.train.string_input_producer(['train.TFRecord'])\n",
    "data, seq_len, label, weight = parse_example(filename_queue)\n",
    "q = tf.PaddingFIFOQueue(\n",
    "    capacity=32, \n",
    "    dtypes=[tf.float32, tf.int64, tf.int64, tf.float32],\n",
    "    shapes=[[None], [], [], []])\n",
    "enqueue_op = q.enqueue([data, seq_len, label, weight])\n",
    "qr = tf.train.QueueRunner(q, [enqueue_op])\n",
    "tf.train.add_queue_runner(qr)\n",
    "\n",
    "batch_size=7\n",
    "batch_op = q.dequeue_many(n=batch_size)\n",
    "res = []\n",
    "with tf.Session() as sess:\n",
    "    print('Sess started')\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    for _ in range(5):\n",
    "        res.append(sess.run(batch_op))\n",
    "        print('fetched batch:', _)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.60436213,  0.60436213,  0.60436213,  0.09040806,  0.29983583,\n",
       "        0.60436213,  0.60436213], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
