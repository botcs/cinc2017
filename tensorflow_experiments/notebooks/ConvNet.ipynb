{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3.5\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import data.ops\n",
    "import model.cnn as cnn\n",
    "import model.rnn as rnn\n",
    "import model.classifier as classifier\n",
    "\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer('gpu', 0, 'device to train on [0]')\n",
    "flags.DEFINE_string('model_def', './hyperparams/test_model.json', 'load hyperparameters from [\"model.json\"]')\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(FLAGS.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths = [\n",
    "    './data/NORMAL_CLASS_REF.TFRecord', \n",
    "    './data/OTHER_CLASS_REF.TFRecord',\n",
    "    './data/ATRIUM_CLASS_REF.TFRecord',\n",
    "    './data/NOISE_CLASS_REF.TFRecord'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = tf.placeholder_with_default(32, [], name='batch_size')\n",
    "(input_op, seq_len, label), input_prods = data.ops.get_even_batch_producer(\n",
    "    paths=paths, batch_size=batch_size)\n",
    "\n",
    "val_input_op, val_seq_len, val_label = data.ops.get_batch_producer(\n",
    "    batch_size=batch_size, path='./data/VALIDATION.TFRecord')\n",
    "\n",
    "validation_feed_dict = {\n",
    "    input_op: val_input_op,\n",
    "    seq_len: val_seq_len,\n",
    "    label: val_label,\n",
    "    batch_size: 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CNN--cnn128x16-256x16-256x16\n",
      "Tensor(\"CNN/Conv1/Conv_dim128_ker16_pool1/Relu:0\", shape=(?, ?, 1, 128), dtype=float32)\n",
      "Tensor(\"CNN/Conv2/Conv_dim256_ker16_pool1/Relu:0\", shape=(?, ?, 1, 256), dtype=float32)\n",
      "Tensor(\"CNN/Conv3/Conv_dim256_ker16_pool1/Relu:0\", shape=(?, ?, 1, 256), dtype=float32)\n",
      "\n",
      "FC--fc\n",
      "Tensor(\"classifier/logits/BiasAdd:0\", shape=(?, 4), dtype=float32)\n",
      "Tensor(\"classifier/predictions:0\", shape=(?, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "cnn_params = {\n",
    "    'out_dims' : [128, 256, 256],\n",
    "    'kernel_sizes' : 64,\n",
    "    'pool_sizes' : 1\n",
    "}\n",
    "c = cnn.model(seq_len=seq_len, input_op=input_op, **cnn_params)\n",
    "\n",
    "#a = tf.transpose(c.output, perm=[0, 2, 1])\n",
    "#a = tf.nn.top_k(a, k=8, sorted=False, name='MAX_POOL').values\n",
    "#a = tf.transpose(a, perm=[0, 2, 1])\n",
    "a = tf.reduce_mean(c.output, axis=1)\n",
    "fc = classifier.model(input_op=a, fc_sizes=[16])\n",
    "\n",
    "logits = fc.logits\n",
    "pred = fc.pred\n",
    "\n",
    "MODEL_PATH = '/tmp/balanced/' + c.name + fc.name\n",
    "MODEL_EXISTS = os.path.exists(MODEL_PATH)\n",
    "if MODEL_EXISTS:\n",
    "    print('Model directory is not empty, removing old files')\n",
    "    shutil.rmtree(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def measure_time(op, feed_dict={}, n_times=10):\n",
    "    with tf.Session() as sess:\n",
    "        print('Sess started')\n",
    "        coord = tf.train.Coordinator()\n",
    "        tf.global_variables_initializer().run()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "        print('Evaluating')\n",
    "        for _ in range(n_times):\n",
    "            t = time.time()\n",
    "            fetch = sess.run(op, feed_dict)\n",
    "            print('Eval time:', time.time() - t)\n",
    "            \n",
    "        print('Closing threads')\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "        return fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sess started\n",
      "Evaluating\n",
      "Eval time: 0.10602164268493652\n",
      "Eval time: 0.026155948638916016\n",
      "Eval time: 0.035398244857788086\n",
      "Eval time: 0.021442413330078125\n",
      "Eval time: 0.008875370025634766\n",
      "Eval time: 0.005707502365112305\n",
      "Eval time: 0.018724679946899414\n",
      "Eval time: 0.00844264030456543\n",
      "Eval time: 0.013972043991088867\n",
      "Eval time: 0.013971090316772461\n",
      "Closing threads\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_time(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "## **Confusion matrix**\n",
    "\n",
    "## **Accuracy operator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('evaluation'):\n",
    "    with tf.name_scope('one_hot_encoding'):\n",
    "        y_oh = tf.cast(tf.equal(\n",
    "            logits, tf.reduce_max(logits, axis=1)[:, None]), tf.float32)\n",
    "\n",
    "        label_oh = tf.one_hot(label, depth=4)\n",
    "    with tf.name_scope('confusion_matrix'):\n",
    "        conf_op = tf.reduce_sum(tf.transpose(\n",
    "            y_oh[..., None], perm=[0, 2, 1]) * label_oh[..., None],\n",
    "            axis=0, name='result')\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        y_tot = tf.reduce_sum(conf_op, axis=0, name='label_class_sum')\n",
    "        label_tot = tf.reduce_sum(conf_op, axis=1, name='pred_class_sum')\n",
    "        correct_op = tf.diag_part(conf_op, name='correct_class_sum')\n",
    "        eps = tf.constant([1e-10] * 4, name='eps')\n",
    "        acc = tf.reduce_mean(2*correct_op / (y_tot + label_tot + eps), name='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class_hist = np.load('./data/class_histogramTRAIN.npy')\n",
    "with tf.name_scope('loss'):\n",
    "    #weight = tf.constant([.1, 1, .2, 3])\n",
    "    weight = tf.constant(1 - np.sqrt(class_hist/class_hist.sum()), name='weights')\n",
    "    weight = tf.gather(weight, label, name='weight_selector')\n",
    "    train_loss = tf.losses.softmax_cross_entropy(\n",
    "        label_oh, logits, weight, scope='weighted_loss')\n",
    "    unweighted_loss = tf.losses.softmax_cross_entropy(\n",
    "        label_oh, logits, scope='unweighted_loss')\n",
    "    \n",
    "    l2_loss = tf.reduce_sum([tf.nn.l2_loss(v, name='L2_reg_loss') \n",
    "                            for v in tf.trainable_variables()])\n",
    "    beta = 0.0001\n",
    "    loss = unweighted_loss + beta * l2_loss\n",
    "#class_hist, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    learning_rate = tf.Variable(initial_value=.001, trainable=False, name='learning_rate')\n",
    "    global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
    "    grad_clip = tf.Variable(initial_value=3., trainable=False, name='grad_clip')\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    gvs = optimizer.compute_gradients(loss)\n",
    "    with tf.name_scope('gradient_clipping'):\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -grad_clip, grad_clip), var) \n",
    "                      for grad, var in gvs]\n",
    "        \n",
    "    opt = optimizer.apply_gradients(capped_gvs, global_step)\n",
    "    \n",
    "    #opt = optimizer.minimize(1-max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter(MODEL_PATH, graph=tf.get_default_graph())\n",
    "sum_ops = []\n",
    "for v in tf.trainable_variables():\n",
    "    sum_ops.append(tf.summary.histogram(v.name[:-2], v))\n",
    "    sum_ops.append(tf.summary.histogram('gradients/'+v.name[:-2], tf.gradients(loss, v)))\n",
    "\n",
    "sum_ops.append(tf.summary.scalar('weighted_loss', loss))\n",
    "sum_ops.append(tf.summary.scalar('unweighted_loss', unweighted_loss))\n",
    "sum_ops.append(tf.summary.scalar('accuracy', acc))\n",
    "sum_ops.append(tf.summary.image('confusion_matrix', conf_op[None, ..., None], max_outputs=10))\n",
    "summaries = tf.summary.merge(sum_ops)\n",
    "eval_summaries = tf.summary.merge([tf.summary.scalar('eval_accuracy', acc), \n",
    "    tf.summary.image('confusion_matrix', conf_op[None, ..., None], max_outputs=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sess started\n",
      "Initializing model\n",
      "Training\n",
      "1/20000 time:40.554707 loss:1.668305 acc:0.100000\n",
      "2/20000 time:4.303600 loss:1.417930 acc:0.361562\n",
      "3/20000 time:4.321317 loss:1.359022 acc:0.503571\n",
      "4/20000 time:4.353394 loss:1.628947 acc:0.264753\n",
      "5/20000 time:4.271616 loss:1.666512 acc:0.333213\n",
      "6/20000 time:39.715581 loss:1.308988 acc:0.436041\n",
      "7/20000 time:4.314733 loss:1.506171 acc:0.361742\n",
      "8/20000 time:4.363753 loss:1.427180 acc:0.335901\n",
      "9/20000 time:4.291524 loss:1.493337 acc:0.194712\n",
      "10/20000 time:4.333869 loss:1.334249 acc:0.437500\n",
      "11/20000 time:40.150571 loss:1.582285 acc:0.289871\n",
      "12/20000 time:4.307957 loss:1.464992 acc:0.408333\n",
      "13/20000 time:4.283029 loss:1.455535 acc:0.256471\n",
      "14/20000 time:4.354297 loss:1.640339 acc:0.267857\n",
      "15/20000 time:4.313282 loss:1.372108 acc:0.490530\n",
      "16/20000 time:27.435669 loss:1.558000 acc:0.344322\n",
      "17/20000 time:4.329582 loss:1.468288 acc:0.356111\n",
      "18/20000 time:4.443154 loss:1.329616 acc:0.319048\n",
      "19/20000 time:4.316531 loss:1.245439 acc:0.293750\n",
      "20/20000 time:4.326881 loss:1.456931 acc:0.316408\n",
      "21/20000 time:4.321815 loss:1.261681 acc:0.416818\n",
      "22/20000 time:39.478971 loss:1.458509 acc:0.336905\n",
      "23/20000 time:39.752718 loss:1.335123 acc:0.398496\n",
      "24/20000 time:4.322015 loss:1.334792 acc:0.539566\n",
      "25/20000 time:4.305689 loss:1.335403 acc:0.437166\n",
      "26/20000 time:4.355512 loss:1.504729 acc:0.373082\n",
      "27/20000 time:4.281441 loss:1.302616 acc:0.314066\n",
      "28/20000 time:4.387744 loss:1.445115 acc:0.245614\n",
      "29/20000 time:4.289613 loss:1.616252 acc:0.484281\n",
      "30/20000 time:28.492205 loss:1.425958 acc:0.306548\n",
      "31/20000 time:4.356746 loss:1.397618 acc:0.306277\n",
      "32/20000 time:4.252097 loss:1.404521 acc:0.332578\n",
      "33/20000 time:4.348392 loss:1.304183 acc:0.435065\n",
      "34/20000 time:4.317798 loss:1.365268 acc:0.443498\n",
      "35/20000 time:4.340604 loss:1.255357 acc:0.555590\n",
      "36/20000 time:4.303160 loss:1.324612 acc:0.353571\n",
      "37/20000 time:4.380569 loss:1.356865 acc:0.333139\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_INT32, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue/Switch:1, producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue/Switch_1:1, producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue/Switch_2/_23, producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue/Switch_3/_25)]]\n",
      "\t [[Node: producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue/_28 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_38_producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
      "\n",
      "Caused by op 'producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue', defined at:\n",
      "  File \"/usr/lib64/python3.5/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib64/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/lib64/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/lib64/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/lib64/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/lib64/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/lib64/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/lib64/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/lib64/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-47c35ffe4091>\", line 4, in <module>\n",
      "    paths=paths, batch_size=batch_size)\n",
      "  File \"/home/csbotos/af_challenge_itk/tensorflow/data/ops.py\", line 126, in get_even_batch_producer\n",
      "    scope='producer_%s'%path\n",
      "  File \"/home/csbotos/af_challenge_itk/tensorflow/data/ops.py\", line 80, in get_batch_producer\n",
      "    name='padded_batch_queue')\n",
      "  File \"/usr/lib/python3.5/site-packages/tensorflow/python/training/input.py\", line 872, in batch\n",
      "    name=name)\n",
      "  File \"/usr/lib/python3.5/site-packages/tensorflow/python/training/input.py\", line 659, in _batch\n",
      "    _enqueue(queue, tensor_list, num_threads, enqueue_many, keep_input)\n",
      "  File \"/usr/lib/python3.5/site-packages/tensorflow/python/training/input.py\", line 634, in _enqueue\n",
      "    control_flow_ops.no_op)] * threads\n",
      "  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1759, in cond\n",
      "    orig_res, res_t = context_t.BuildCondBranch(fn1)\n",
      "  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1660, in BuildCondBranch\n",
      "    r = fn()\n",
      "  File \"/usr/lib/python3.5/site-packages/tensorflow/python/training/input.py\", line 633, in <lambda>\n",
      "    lambda: enqueue_fn(tensor_list),\n",
      "  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 333, in enqueue\n",
      "    self._queue_ref, vals, name=scope)\n",
      "  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1569, in _queue_enqueue_v2\n",
      "    name=name)\n",
      "  File \"/usr/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n",
      "    op_def=op_def)\n",
      "  File \"/usr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"/usr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n",
      "    self._traceback = _extract_stack()\n",
      "\n",
      "CancelledError (see above for traceback): Enqueue operation was cancelled\n",
      "\t [[Node: producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_INT32, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue/Switch:1, producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue/Switch_1:1, producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue/Switch_2/_23, producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue/Switch_3/_25)]]\n",
      "\t [[Node: producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue/_28 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_38_producer_./data/NORMAL_CLASS_REF.TFRecord/padded_batch_queue/cond/padding_fifo_queue_enqueue\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1a35c41eb612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mfetch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         print('%d/%d'%(step, TRAIN_STEPS), \n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(keep_checkpoint_every_n_hours=1)\n",
    "#with open('test.txt', 'w') as f:\n",
    "    #metagraph = saver.export_meta_graph(as_text=True)\n",
    "    #f.write(str(metagraph.ListFields()))\n",
    "    \n",
    "TRAIN_STEPS = 20000\n",
    "with tf.Session() as sess:\n",
    "    print('Sess started')\n",
    "    \n",
    "    print('Initializing model')\n",
    "    tf.global_variables_initializer().run()\n",
    "        \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    print('Training')\n",
    "    for i in range(TRAIN_STEPS):\n",
    "        t = time.time()\n",
    "        fetch = sess.run([opt, loss, acc, global_step])\n",
    "        step = fetch[-1]\n",
    "        print('%d/%d'%(step, TRAIN_STEPS), \n",
    "              'time:%f'%(time.time()-t), \n",
    "              'loss:%f'%fetch[1],\n",
    "              'acc:%f'%fetch[2]\n",
    "              )\n",
    "        if step % 50 == 0:\n",
    "            print('Evaluating TRAIN summaries...')\n",
    "            train_writer.add_summary(summaries.eval(), global_step=fetch[-1])\n",
    "        if step % 100 == 0:\n",
    "            print('Evaluating VALIDATION summaries...')\n",
    "            train_writer.add_summary(\n",
    "                eval_summaries.eval(validation_feed_dict), \n",
    "                global_step=fetch[-1])\n",
    "        if step % 250 == 0:\n",
    "            print('Saving model...')\n",
    "            print(saver.save(sess, MODEL_PATH, global_step=fetch[-1]))\n",
    "    \n",
    "    print('Ending, closing producer threads')\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
